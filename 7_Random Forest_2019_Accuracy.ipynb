{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Introduction\n",
    "So far, we have learned about decision trees, and looked at ways to reduce overfitting. The most powerful method to reduce decision tree overfitting is called the random forest algorithm. In this notebook, we'll learn how to construct and apply random forests.\n",
    "\n",
    "We've been using the dataset, Carseats.csv, in this example. The data contains sales information about car seat sales in 400 stores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Ensemble Models\n",
    "A random forest is a kind of **ensemble model**. Ensembles combine the predictions of multiple models to create a more accurate final prediction. We'll make a simple ensemble to see how it works.\n",
    "\n",
    "We'll create two decision trees with slightly different parameters:\n",
    "\n",
    "one with min_samples_leaf set to 2\n",
    "one with max_depth set to 5\n",
    "and check their accuracy separately. In the next screen, we'll combine their predictions and compare the combined accuracy with either tree's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Sales</th>\n",
       "      <th>CompPrice</th>\n",
       "      <th>Income</th>\n",
       "      <th>Advertising</th>\n",
       "      <th>Population</th>\n",
       "      <th>Price</th>\n",
       "      <th>ShelveLoc</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Urban</th>\n",
       "      <th>US</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>9.50</td>\n",
       "      <td>138</td>\n",
       "      <td>73</td>\n",
       "      <td>11</td>\n",
       "      <td>276</td>\n",
       "      <td>120</td>\n",
       "      <td>Bad</td>\n",
       "      <td>42</td>\n",
       "      <td>17</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>11.22</td>\n",
       "      <td>111</td>\n",
       "      <td>48</td>\n",
       "      <td>16</td>\n",
       "      <td>260</td>\n",
       "      <td>83</td>\n",
       "      <td>Good</td>\n",
       "      <td>65</td>\n",
       "      <td>10</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>10.06</td>\n",
       "      <td>113</td>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>269</td>\n",
       "      <td>80</td>\n",
       "      <td>Medium</td>\n",
       "      <td>59</td>\n",
       "      <td>12</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>7.40</td>\n",
       "      <td>117</td>\n",
       "      <td>100</td>\n",
       "      <td>4</td>\n",
       "      <td>466</td>\n",
       "      <td>97</td>\n",
       "      <td>Medium</td>\n",
       "      <td>55</td>\n",
       "      <td>14</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4.15</td>\n",
       "      <td>141</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>340</td>\n",
       "      <td>128</td>\n",
       "      <td>Bad</td>\n",
       "      <td>38</td>\n",
       "      <td>13</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Sales  CompPrice  Income  Advertising  Population  Price  \\\n",
       "0           1   9.50        138      73           11         276    120   \n",
       "1           2  11.22        111      48           16         260     83   \n",
       "2           3  10.06        113      35           10         269     80   \n",
       "3           4   7.40        117     100            4         466     97   \n",
       "4           5   4.15        141      64            3         340    128   \n",
       "\n",
       "  ShelveLoc  Age  Education Urban   US  \n",
       "0       Bad   42         17   Yes  Yes  \n",
       "1      Good   65         10   Yes  Yes  \n",
       "2    Medium   59         12   Yes  Yes  \n",
       "3    Medium   55         14   Yes  Yes  \n",
       "4       Bad   38         13   Yes   No  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instructions\n",
    "'''\n",
    "- Fit both clf and clf2 to the data.\n",
    "- Make predictions on the test set predictors (test[columns]) using both clf and clf2.\n",
    "'''\n",
    "\n",
    "df3 = pd.read_csv('Data/Carseats.csv').dropna()\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative Predictors\n",
    "The Carseats data includes qualitative predictors such as Shelveloc, an indicator of the quality of the shelving location—that is, the space within a store in which the car seat is displayed—at each location. The predictor Shelveloc takes on three possible values, Bad, Medium, and Good.  We will create a ShelveLocGood dummy variable that takes on a value of 1 if the shelving location is good, and 0 otherwise. We will also create a ShelveLocMedium dummy variable that equals 1 if the shelving location is medium, and 0 otherwise. A bad shelving location corresponds to a zero for each of the two dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Sales</th>\n",
       "      <th>CompPrice</th>\n",
       "      <th>Income</th>\n",
       "      <th>Advertising</th>\n",
       "      <th>Population</th>\n",
       "      <th>Price</th>\n",
       "      <th>ShelveLoc</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Urban</th>\n",
       "      <th>US</th>\n",
       "      <th>HighSales</th>\n",
       "      <th>Bad</th>\n",
       "      <th>Good</th>\n",
       "      <th>Medium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>9.50</td>\n",
       "      <td>138</td>\n",
       "      <td>73</td>\n",
       "      <td>11</td>\n",
       "      <td>276</td>\n",
       "      <td>120</td>\n",
       "      <td>Bad</td>\n",
       "      <td>42</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>11.22</td>\n",
       "      <td>111</td>\n",
       "      <td>48</td>\n",
       "      <td>16</td>\n",
       "      <td>260</td>\n",
       "      <td>83</td>\n",
       "      <td>Good</td>\n",
       "      <td>65</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>10.06</td>\n",
       "      <td>113</td>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>269</td>\n",
       "      <td>80</td>\n",
       "      <td>Medium</td>\n",
       "      <td>59</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>7.40</td>\n",
       "      <td>117</td>\n",
       "      <td>100</td>\n",
       "      <td>4</td>\n",
       "      <td>466</td>\n",
       "      <td>97</td>\n",
       "      <td>Medium</td>\n",
       "      <td>55</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4.15</td>\n",
       "      <td>141</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>340</td>\n",
       "      <td>128</td>\n",
       "      <td>Bad</td>\n",
       "      <td>38</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Sales  CompPrice  Income  Advertising  Population  Price  \\\n",
       "0           1   9.50        138      73           11         276    120   \n",
       "1           2  11.22        111      48           16         260     83   \n",
       "2           3  10.06        113      35           10         269     80   \n",
       "3           4   7.40        117     100            4         466     97   \n",
       "4           5   4.15        141      64            3         340    128   \n",
       "\n",
       "  ShelveLoc  Age  Education  Urban  US  HighSales  Bad  Good  Medium  \n",
       "0       Bad   42         17      1   1          1    1     0       0  \n",
       "1      Good   65         10      1   1          1    0     1       0  \n",
       "2    Medium   59         12      1   1          1    0     0       1  \n",
       "3    Medium   55         14      1   1          0    0     0       1  \n",
       "4       Bad   38         13      1   0          0    1     0       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert sales to binary\n",
    "df3['HighSales'] = df3.Sales.map(lambda x: 1 if x>8 else 0)\n",
    "\n",
    "df3.Urban = df3.Urban.map({'No':0, 'Yes':1})\n",
    "df3.US = df3.US.map({'No':0, 'Yes':1})\n",
    "\n",
    "# convert categorical variables to dummy variables\n",
    "ShelveLoc = pd.get_dummies(df3['ShelveLoc'])\n",
    "\n",
    "# Join the dummy variables to the main dataframe\n",
    "df3_new = pd.concat([df3, ShelveLoc], axis=1)\n",
    "\n",
    "df3_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['CompPrice', 'Income', 'Advertising', 'Population', 'Price', 'Age',\n",
      "       'Education', 'Urban', 'US', 'Good', 'Medium'],\n",
      "      dtype='object')\n",
      "0.71\n",
      "0.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pzx0002\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X = df3_new.drop(['Sales', 'HighSales', 'ShelveLoc', 'Bad', 'Unnamed: 0'], axis=1)\n",
    "y = df3_new.HighSales\n",
    "\n",
    "x_columns = X.columns\n",
    "print(x_columns)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, random_state=0)\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=1, min_samples_leaf=2)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "clf2 = DecisionTreeClassifier(random_state=1, max_depth=5)\n",
    "clf2.fit(X_train, y_train)\n",
    "\n",
    "predictions = clf.predict(X_test)\n",
    "print(accuracy_score(y_test, predictions))\n",
    "\n",
    "predictions = clf2.predict(X_test)\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Combining Our Predictions\n",
    "When we have multiple classifiers making predictions, we can treat each set of predictions as a column in a matrix. \n",
    "\n",
    "Here's an example where we have Decision Tree 1 (DT1), Decision Tree 2 (DT2), and DT3:\n",
    "\n",
    "DT1  |  DT2 | DT3\n",
    "--- | --- | --- \n",
    "0 | 1 | 0\n",
    "1 | 1 | 1\n",
    "0 | 0 | 1\n",
    "1 | 0 | 0\n",
    "\n",
    "When we add more models to our ensemble, we just add more columns to the combined predictions. Ultimately, we don't want this matrix, though -- we want one prediction per row in the training data. To do this, we'll need to create rules to turn each row of our matrix of predictions into a single number.\n",
    "\n",
    "We want to create a Final Prediction vector:\n",
    "\n",
    "DT1  |   DT2  |  DT3  |  Final Prediction\n",
    "--- | --- | --- \n",
    "0    |   1  |    0   |   0\n",
    "1   |    1  |    1   |   1\n",
    "0   |    0    |  1  |    0\n",
    "1  |     0   |   0  |    0\n",
    "\n",
    "There are many ways to get from the output of multiple models to a final vector of predictions. One method is **majority voting**, where each classifier gets a \"vote\", and the most commonly voted value for each row wins. This only works if there are more than 2 classifiers (and ideally an odd number so we don't have to write a rule to break ties). Majority voting is what we applied in the example above.\n",
    "\n",
    "Since in the last screen we only had two classifiers, we'll have to use a different method to combine predictions. We'll take the mean of all the items in a row. Right now, we're using the predict method, which returns either 0 or 1. predict returns something like this:\n",
    "\n",
    "0\n",
    "1\n",
    "0\n",
    "1\n",
    "\n",
    "\n",
    "We can instead use the predict_proba method, which will predict a probability from 0 to 1 that a given class is the right one for a row. Since 0 and 1 are our two classes, we'll get a matrix with as many rows as the income dataframe and 2 columns. predict_proba will return something like this:\n",
    "\n",
    "\n",
    "0  |  1\n",
    "--- | --- \n",
    ".7  | .3\n",
    ".2  | .8\n",
    ".1  | .9\n",
    "\n",
    "\n",
    "Each row will correspond to a prediction. The first column is the probability that the prediction is a 0, the second column is the probability that the prediction is a 1. Each row adds up to 1.\n",
    "\n",
    "If we just take the second column, we get the average value that the classifier would predict for that row. If there's a .9 probability that the correct classification is 1, we can use the .9 as the value the classifier is predicting. This will give us a continuous output in a single vector instead of just 0 or 1.\n",
    "\n",
    "We can then add all of the vectors we get through this method together and divide by the number of vectors to get the mean prediction by all the members of the ensemble. We can then round off to get 0 or 1 predictions.\n",
    "\n",
    "If we use the predict_proba method on both classifiers from the last screen to generate probabilities, take the mean for each row, and then round the results, we'll get ensemble predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.73\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "<Instruction>\n",
    "Add predictions and predictions2, then divide by 2 to get the mean.\n",
    "Use numpy.round to round all of the resulting predictions.\n",
    "Print the resulting AUC score between the actual values and the predictions.\n",
    "'''\n",
    "\n",
    "predictions = clf.predict_proba(X_test)[:,1]\n",
    "predictions2 = clf2.predict_proba(X_test)[:,1]\n",
    "combined = (predictions + predictions2) / 2\n",
    "rounded = np.round(combined)\n",
    "\n",
    "print(accuracy_score(y_test, rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Why Ensembling Works\n",
    "As we can see from the previous screen, the combined predictions of the two trees had a higher AUC than either tree:\n",
    "\n",
    "settings|test AUC\n",
    "--- | --- \n",
    "min_samples_leaf: 2|0.71\n",
    "max_depth: 5|0.67\n",
    "combined predictions|0.73\n",
    "\n",
    "\n",
    "To intuitively understand why this makes sense, think about two people at the same talent level. One learned programming in college. The other learned on their own.\n",
    "\n",
    "If you give both of them a project, since they both have different knowledge and experience, they'll both approach it in slightly different ways. They may both produce code that achieves the same result, but one may run faster in certain areas. The other may have a better interface. Even though both of them have about the same talent level, because they approach the problem differently, their solutions are stronger in different areas.\n",
    "\n",
    "If we combine the best parts of both of their projects, we'll end up with a stronger combined project.\n",
    "\n",
    "Ensembling is the exact same. Both models are approaching the problem slightly differently, and building a different tree because we used different parameters for each. Each tree makes different predictions in different areas. Even though both trees have about the same accuracy, when we combine them, the result is stronger because it leverages the strengths of both approaches.\n",
    "\n",
    "The more \"diverse\", or dissimilar, the models used to construct an ensemble, the stronger the combined predictions will be (assuming that all models have about the same accuracy). Ensembling a decision tree and a logistic regression model, which use very different approaches to arrive at their answers, will result in stronger predictions than ensembling two decision trees with similar parameters.\n",
    "\n",
    "On the other side, if the models you ensemble are very similar in how they make predictions, you'll get a negligible boost from ensembling.\n",
    "\n",
    "Ensembling models with very different accuracies will not generally improve your accuracy. Ensembling a model with a .75 AUC and a model with a .85 AUC on a test set will usually result in an AUC somewhere in between the two original values. There's a way around this which we'll discuss later on, called weighting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5: Bagging\n",
    "A random forest is an ensemble of decision trees. If we don't make any modifications to the trees and follow the same building algorithm, each tree will be the exact same, so we'll get no boost when we ensemble them. In order to make ensembling effective, we have to introduce variation into each individual decision tree model.\n",
    "\n",
    "If we introduce variation, each tree will be be constructed slightly differently, and therefore will make different predictions. This variation is why the word \"random\" is in \"random forest\".\n",
    "\n",
    "There are two main ways to introduce variation in a random forest -- bagging and random feature subsets. We'll dive into bagging first.\n",
    "\n",
    "In a random forest, each tree is trained on a random sample of the data, or a \"bag\". This sampling is performed with replacement. When we sample with replacement, after we select a row from the data we're sampling, we put the row back in the data so it can be picked again. Some rows from the original data may appear in the \"bag\" multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.74\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, BaggingRegressor, RandomForestRegressor\n",
    "\n",
    "# we can achieve the above two tasks using the following codes\n",
    "# Bagging: using all features\n",
    "rfc1 = RandomForestClassifier(max_features=11, random_state=1)\n",
    "rfc1.fit(X_train, y_train)\n",
    "pred1 = rfc1.predict(X_test)\n",
    "print(accuracy_score(y_test, pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6: Selecting Random Features\n",
    "With the bagging example from the previous screen, we gained some accuracy over a single decision tree. We achieved an Accuracy score of around 0.74 with bagging.\n",
    "\n",
    "settings|test AUC\n",
    "--- | --- \n",
    "min_samples_leaf: 2|0.71\n",
    "max_depth: 5|0.67\n",
    "combined predictions|0.73\n",
    "min_samples_leaf: 2, with bagging|0.74\n",
    "\n",
    "In this section, we'll only evaluate a constrained set of features, selected randomly. This introduces variation into the trees, and makes for more powerful ensembles.\n",
    "\n",
    "We can also repeat our random subset selection process in scikit-learn. We just set the splitter parameter on DecisionTreeClassifier to \"random\", and the max_features parameter to \"auto\". If we have N columns, this will pick a subset of features of size sqrt(N)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 1 feature(s) equals 0.73\n",
      "Accuracy for 2 feature(s) equals 0.765\n",
      "Accuracy for 3 feature(s) equals 0.74\n",
      "Accuracy for 4 feature(s) equals 0.695\n",
      "Accuracy for 5 feature(s) equals 0.74\n",
      "Accuracy for 6 feature(s) equals 0.705\n",
      "Accuracy for 7 feature(s) equals 0.73\n",
      "Accuracy for 8 feature(s) equals 0.745\n",
      "Accuracy for 9 feature(s) equals 0.755\n",
      "Accuracy for 10 feature(s) equals 0.73\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, BaggingRegressor, RandomForestRegressor\n",
    "\n",
    "# play around with the setting for max_features\n",
    "for i in range(1, 11):\n",
    "    rfc2 = RandomForestClassifier(max_features=i, random_state=1)\n",
    "    rfc2.fit(X_train, y_train)\n",
    "    pred2 = rfc2.predict(X_test)\n",
    "    print(\"Accuracy for %s feature(s) equals %s\" %(i, accuracy_score(y_test, pred2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CompPrice', 0.15141418751502395),\n",
       " ('Income', 0.12372719124461444),\n",
       " ('Advertising', 0.10860891444101148),\n",
       " ('Population', 0.08540184584772219),\n",
       " ('Price', 0.3003808724403305),\n",
       " ('Age', 0.13501976621014836),\n",
       " ('Education', 0.030790401661152867),\n",
       " ('Urban', 0.0014078062858550661),\n",
       " ('US', 0.0054020015121781315),\n",
       " ('Good', 0.048464509864021586),\n",
       " ('Medium', 0.009382502977941352)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View a list of the features and their importance scores\n",
    "list(zip(X_train, rfc1.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ecf6b7fbe0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We can also plot the important features\n",
    "\n",
    "feat_importances = pd.Series(rfc1.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(5).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ecf6ccb358>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feat_importances = pd.Series(rfc2.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(5).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7: When To Use Random Forests\n",
    "\n",
    "** Putting It All Together:**\n",
    "\n",
    "settings|test AUC\n",
    "--- | --- \n",
    "min_samples_leaf: 2|0.71\n",
    "max_depth: 5|0.67\n",
    "combined predictions|0.73\n",
    "min_samples_leaf: 2, with bagging|0.74\n",
    "min_samples_leaf: 2, with bagging and random subsets|0.77\n",
    "\n",
    "\n",
    "The random forest algorithm is incredibly powerful, but isn't applicable to all tasks. The main strengths of a random forest are:\n",
    "\n",
    "Very accurate predictions -- Random forests achieve near state of the art performance on many machine learning tasks. Along with neural networks and gradient boosted trees, they are typically one of the top performing algorithms.\n",
    "Resistance to overfitting -- due to how they're constructed, random forests are fairly resistant to overfitting. Parameters like max_depth still have to be set and tweaked, though.\n",
    "\n",
    "The main weaknesses are:\n",
    "Hard to interpret -- because we've averaging the results of many trees, it can be hard to figure out why a random forest is making predictions the way it is.\n",
    "Longer creation time -- making two trees takes twice as long as making one, 3 takes three times as long, and so on. Luckily, we can exploit multicore processors to parallelize tree construction. \n",
    "\n",
    "Given these tradeoffs, it makes sense to use random forests in situations where accuracy is of the utmost importance, and being able to interpret or explain the decisions the model is making isn't key. In cases where time is of the essence, or interpretability is important, a single decision tree may be a better choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Reading\n",
    "\n",
    "\n",
    "#### Introducing Partial Dependence Plots: Python PDPbox\n",
    "\n",
    "When using black box machine learning algorithms like random forest and boosting, it is hard to understand the relations between predictors and model outcome. For example, in terms of random forest, all we get is the feature importance. Although we can know which feature is significantly influencing the outcome based on the importance calculation, it really sucks that we don’t know in which direction it is influencing. And in most of the real cases, the effect is non-monotonic. We need some powerful tools to help understanding the complex relations between predictors and model prediction. \n",
    "\n",
    "https://towardsdatascience.com/introducing-pdpbox-2aa820afd312\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
